-- Nel data parallelism un singolo modello viene replicato su device multipli o macchine multiple. Ognuna di esse processa un batch di dati differente e poi fanno un merge dei propri risultati. Ci sono diverse varianti per esempio se le macchine rimangono in sync dopo ogni batch o rimangono loosely coupled.

-- Nel model parallelism parto differenti di un singolo modello runnano su device diversi, processando una singola batch di dati insieme. Funziona meglio con modelli che hanno più branch.


-- In Keras sync data parallelism: usando le API di tf.distribute per l'allenamento.

1) Su gpu multiple nella stessa machina -> single host multi device training

2) Su un cluster di macchine, ognuna con una o più gpu (la documentazione di keras si riferisce quasi solo a gpu ma è possibile usare la cpu andando a dichiarare a livello di codice il numero di gpu = -1)

Per entrambe le tecniche ci sono alcuni punti chiave in comune per quanto riguarda l'allenamento (poi l'implementazione è un altro paio di maniche):

    - La batch di dati corrente (global batch) viene divisa in batch diverse in base al numero di repliche (local batches). EX: batch da 512 elementi con 6 repliche -> 64 elementi per batch locale.

    - Ognuna delle repliche processa la pripria batch locale facendo un forward pass, backward pass e dando in output il gradiente dei pesi con attenzione al loss del model nella local batch.

    - Gli aggiornamenti di peso sono poi mergiati su tutte le repliche, essendo che ciò avviene alla fine di ogni step (sottoinsieme di un epoca) le repliche rimangono in sync.

In pratica il processo di update sincrono dei pesi della replica è gestito a livello di ogni variabile peso usando un oggetto chiamato mirrored variable. Dunque come da ipotesi durante il training ad ogni step vengono confrontate e scelte le variabili peso "migliori" e poi propagate a tutte le repliche. In questo modo a fine allenamento ottengo un solo modello compilato che è l'unione dei parametri migliori. Per quanto riguarda la fase di "utilizzo" vero e proprio di questo modello le repliche per l'allenamento sono riutilizzabili però di fatto è come se avessi una sola rete visto che sono tutte uguali.

Per quanto riguarda l'approccio cluster se si usano container su pc diversi, dunque di base macchine diverse dall'host, è molto facile (essendo documentato cosi sembra almeno):
    - Si settuppa un cluster usando delle variabili d'ambiente (TF_CONFIG) su ognuno dei worker in modo da indicargli il suo ruolo e come communicare con i suoi simili. 
    - Su ognuno dei worker si crea e si compila il modello in base allo scope datto da un oggetto MultiWorkerMirroredStrategy (indica la strategia per il mirroring delle variabili, come per il single host descrive anche la divsione delle batch and co).

La prima prova è di provare a rendere dei container in locale "macchine diverse" per il training distribuito. Per test pensavo a 2-3 container tutti con la stessa immagine. 


TEST HOST-WORKER SYNC DATA: SUCCESSO
Usando due container (host e worker) sono risucito a farli communicare (con porte esposte nel dockerfile e network_mode:host in docker-compose) ed eseguire due epoche in modo sincrono. I due container sono quasi identici: l'unica differenza è l'index in TF_CONFIG (0 per host, 1 per worker).
(Nuova speranza per Mesh Tensorflow?)

TODO: Integrare la rete main (cats&dogs) e fare prove per sync/async data e model parallelism...e dare una nuova chance a Mesh Tensorflow.

INTEGRAZIONE MAIN DATA // SYNC:
Modificando un pò la parte di codice, spostando la compliazione e il training del modello nello scope della strategia, sono riuscito a far andare il training sui due container precedenti: host-worker e worker. Come previsto il tempo di training è sempre lo stesso (saranno pure due ma la potenza di calcolo rimane sempre la stessa), circa 28 min. Essendo una stategia pensata al multi-device i veri benefici si vedono solo su un cluster "reale" .
- COME FUNZIONA:
Si creano dei container già pronti con tutto il necessario per far partire il codice. Ognuno runna lo stesso file python con la solo differenza che per ognuno la variabile index cambia (ex: host-worker 0 worker 1 worker 2 ecc...). Inoltre tutte i container possiedono gli stessi dataset di allenamento e validazione. L'host-worker è il primo a partire e una volta definita la strategia e il numero di container lavoratori (attraverso l'esposizione delle porte) divide la batch di allenamento (una batch è l'insieme degli elementi usati dal modello) andando ad indicare ad ogni container quale sottoinsieme prendere. Tutte le macchine eseguono la fase di training insieme. Ad ogni fine step vengono confrontati tutti i pesi generati da ogni singolo container sulla propria sotto-batch e fatti testare su un set di validazione. Vengono considerati solo i pesi migliori creando cosi un "nuovo" modello che viene distribuito a tutti i container, creando una stessa base per tutti prima del prossimo step di training. A fine di tutti gli step e le epoche viene cosi generato il modello "migliore".  

