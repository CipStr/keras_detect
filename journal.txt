-- Nel data parallelism un singolo modello viene replicato su device multipli o macchine multiple. Ognuna di esse processa un batch di dati differente e poi fanno un merge dei propri risultati. Ci sono diverse varianti per esempio se le macchine rimangono in sync dopo ogni batch o rimangono loosely coupled.

-- Nel model parallelism parto differenti di un singolo modello runnano su device diversi, processando una singola batch di dati insieme. Funziona meglio con modelli che hanno più branch.


-- In Keras sync data parallelism: usando le API di tf.distribute per l'allenamento.

1) Su gpu multiple nella stessa machina -> single host multi device training

2) Su un cluster di macchine, ognuna con una o più gpu (la documentazione di keras si riferisce quasi solo a gpu ma è possibile usare la cpu andando a dichiarare a livello di codice il numero di gpu = -1)

Per entrambe le tecniche ci sono alcuni punti chiave in comune per quanto riguarda l'allenamento (poi l'implementazione è un altro paio di maniche):

    - La batch di dati corrente (global batch) viene divisa in batch diverse in base al numero di repliche (local batches). EX: batch da 512 elementi con 6 repliche -> 64 elementi per batch locale.

    - Ognuna delle repliche processa la pripria batch locale facendo un forward pass, backward pass e dando in output il gradiente dei pesi con attenzione al loss del model nella local batch.

    - Gli aggiornamenti di peso sono poi mergiati su tutte le repliche, essendo che ciò avviene alla fine di ogni step (sottoinsieme di un epoca) le repliche rimangono in sync.

In pratica il processo di update sincrono dei pesi della replica è gestito a livello di ogni variabile peso usando un oggetto chiamato mirrored variable. Dunque come da ipotesi durante il training ad ogni step vengono confrontate e scelte le variabili peso "migliori" e poi propagate a tutte le repliche. In questo modo a fine allenamento ottengo un solo modello compilato che è l'unione dei parametri migliori. Per quanto riguarda la fase di "utilizzo" vero e proprio di questo modello le repliche per l'allenamento sono riutilizzabili però di fatto è come se avessi una sola rete visto che sono tutte uguali.

Per quanto riguarda l'approccio cluster se si usano container su pc diversi, dunque di base macchine diverse dall'host, è molto facile (essendo documentato cosi sembra almeno):
    - Si settuppa un cluster usando delle variabili d'ambiente (TF_CONFIG) su ognuno dei worker in modo da indicargli il suo ruolo e come communicare con i suoi simili. 
    - Su ognuno dei worker si crea e si compila il modello in base allo scope datto da un oggetto MultiWorkerMirroredStrategy (indica la strategia per il mirroring delle variabili, come per il single host descrive anche la divsione delle batch and co).

La prima prova è di provare a rendere dei container in locale "macchine diverse" per il training distribuito. Per test pensavo a 2-3 container tutti con la stessa immagine. 


TEST HOST-WORKER SYNC DATA: SUCCESSO
Usando due container (host e worker) sono risucito a farli communicare (con porte esposte nel dockerfile e network_mode:host in docker-compose) ed eseguire due epoche in modo sincrono. I due container sono quasi identici: l'unica differenza è l'index in TF_CONFIG (0 per host, 1 per worker).
(Nuova speranza per Mesh Tensorflow?)

TODO: Integrare la rete main (cats&dogs) e fare prove per sync/async data e model parallelism...e dare una nuova chance a Mesh Tensorflow.

INTEGRAZIONE MAIN DATA // SYNC:
Modificando un pò la parte di codice, spostando la compliazione e il training del modello nello scope della strategia, sono riuscito a far andare il training sui due container precedenti: host-worker e worker. Come previsto il tempo di training è sempre lo stesso (saranno pure due ma la potenza di calcolo rimane sempre la stessa), circa 28 min. Essendo una stategia pensata al multi-device i veri benefici si vedono solo su un cluster "reale" .
- COME FUNZIONA:
Si creano dei container già pronti con tutto il necessario per far partire il codice. Ognuno runna lo stesso file python con la solo differenza che per ognuno la variabile index cambia (ex: host-worker 0 worker 1 worker 2 ecc...). Inoltre tutte i container possiedono gli stessi dataset di allenamento e validazione. L'host-worker è il primo a partire e una volta definita la strategia e il numero di container lavoratori (attraverso l'esposizione delle porte) divide la batch di allenamento (una batch è l'insieme degli elementi usati dal modello) andando ad indicare ad ogni container quale sottoinsieme prendere. Tutte le macchine eseguono la fase di training insieme. Ad ogni fine step vengono confrontati tutti i pesi generati da ogni singolo container sulla propria sotto-batch e fatti testare su un set di validazione. Vengono considerati solo i pesi migliori creando cosi un "nuovo" modello che viene distribuito a tutti i container, creando una stessa base per tutti prima del prossimo step di training. A fine di tutti gli step e le epoche viene cosi generato il modello "migliore". 

ESPERIMENTO ASYNC DATA //: SEMI-SUCCESSO
Cambiando strategia in ParameterServerStrategy e definendo due ruoli aggiuntivi (chief e ps "parameter server") sembra essere riuscito il tentaivo. L'unico dubbio è dovuto da alcuni errori di tensorflow su end of files nei due worker ma suppongo siano divuti al fine allnemanto e qualche eccezione non gestita, dopotutta questa strategia è ancora experimental per tensorflow.
-COME FUNZIONA:
Esistono 3 ruoli diversi (4 con l'evaluatore che non ho implementato): chief, ps e worker. Il chief è colui che gestisce la politica di divisione delle batch ed in generale supervisiona lo scope della strategia. Il parameter server contiene tutti i parametri del modello e quando un worker finisce la sua epoca di allenamento decide se accettare o meno i nuovi parametri calcolati (l'obiettivo è sempre quello di ottenere il modello migliore). I worker eseguono i cicli di training sulle batch indicate dal chief. Ognuono agisce in modo proprio e asincrono riseptto ai suoi simili. A fine epoca di training comunica con il sever in modo da potenzialmente mandare i propri parametri allenati. 
La comunicazione di tutti gli agenti (more like contianers here) avviene attraveso il cluster resolver che tiene memeoria dei vari indirizzi e porte dei singoli (simile ad async con l'uso di porte diverse in localhost). Quando tutti i worker finiscono le proprie epoche di training lo chief costruisce il modello prendendo i parametri gia allenati presenti nel parameter server e salva il modello. 

TODO: capire divisione delle batch da parte di chief e se i worker una volta che non mandano i propri pesi al ps perchè ritenuti peggiori rispetto a quelli già presenti se prendono i pesi dal ps per avere una nuova base di partenza migliore. 

UPDATE: Alla fine l'erroe era vero e importante. La risoluzione è stata fare repeat(n epoche) dei dataset di training e valutazione prima del prefetch(). Il problema era dovuto al fatto che i worker partivano corettamente ma una volta esaurito il numero di dati nelle batch durante la prima epoca mandavano degli errori di OutOfRange per il graph e diventavano "hanged" cioè sospesi. Con i worker sospesi anche i parameter server diventavano hanged in quantonon ricevevano più dati, ergo una cascata di errori. Risolto il 07/09 alle 2 del mattino.
Per quanto riguarda la divisione delle batch se ne occupa il chief ma ci possono essere batch "simili" cioè con elementi in comune. Per questo è consigliato fare una shuffle e repeat delle batch nei worker. Per quanto riguarda la questione pesi i worker inviano i propri pesi ma ne prendono anche dal parameter server. É asincrono si ma il concetto di "stessa base" è applicabile anche qua.

TODO: Model // --> Mesh Tensorflow?

UPDATE: Model e mesh tensorflow
A causa di problemi tecnici (non so come usare mesh tensorflow) non sono riuscito a rendere questa parte nel pratico. Mi sembra di capire che questa libreria non usa i tensor ma bensi le mesh per poter realizzare il model parallelism:
- Tensor: sono array multi dimensionali con un tipo uniforme (dtype). Molto molto simili agli array di numpy. Sono immutabili.
- Mesh: è un array multi dimensionale di processori connessi da una rete. Ogni tensore è distribuito (diviso o replicato) in tutti i processori della mesh. 

Di base non si può "dividere" il modello facilemente con Tensorflow, di fatto non vi esistono strategie già costruite come per il data parallelism. Il problema nel creare un problema "custom" divisible sta nella atomicità dei passaggi: di fatto non ha senso allenare solo una parte di modello alla volta. L'output di un'operazione su un tensore è fondamentale in quanto è l'input per la successiva. È una pipeline. Con mesh un utente esperto può specificare qualisasi dimensione del tensore da dividere in tutte le dimensioni della mesh. Semplifica il tutto in quanto provare ad assegnare operazioni differenti a device differenti riuslta in un grafo sconnesso, mesh si occupa proprio di mantenere un grafo connesso con una struttura logica ben definita.
Più nel preciso Mesh Tensorflow adotta un approccio ispirato al data-parallelism sincrono in quanto tutti i processori (o device) sono coinvolti nelle operazioni. Come per gli esempi in Tensorflow si tratta di un SPMD (Single Program Multiple Devices). Mesh si occupa anche della communicazione dei vari processori.

